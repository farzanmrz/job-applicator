# Custom Agents

For scenarios where an agent's behavior extends beyond predefined patterns, the AutoGen framework facilitates the creation of custom agents. All agents in AgentChat are designed to inherit from the `BaseChatAgent` class. This foundational class mandates the implementation of specific abstract methods and attributes to ensure consistent agent functionality:

*   **`on_messages()`**: This abstract method is pivotal, defining the agent's response logic when it receives messages. It is invoked when the agent is prompted to generate a response via the `run()` method and is expected to return a `Response` object.
*   **`on_reset()`**: This abstract method is responsible for returning the agent to its initial, un-mutated state. It is automatically called when a request is made to reset the agent.
*   **`produced_message_types`**: This attribute is a list specifying all potential `BaseChatMessage` types that the agent is capable of generating as part of its response.

Furthermore, the framework offers an optional method, `on_messages_stream()`, which can be implemented to stream messages as they are actively generated by the agent. This method is invoked by `run_stream()` to facilitate message streaming. Should `on_messages_stream()` not be explicitly implemented, the agent will gracefully fall back to a default implementation. This default behavior involves calling the `on_messages()` method internally and then yielding all messages contained within the resulting response.

### CountDownAgent

As an illustrative example, let's construct a straightforward agent, `CountDownAgent`, designed to count down from a specified integer to zero. This agent will progressively produce a stream of messages, each indicating the current count.

```python
from typing import AsyncGenerator, List, Sequence
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.base import Response
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, TextMessage
from autogen_core import CancellationToken

class CountDownAgent(BaseChatAgent):
    def __init__(self, name: str, count: int = 3):
        super().__init__(name, "A simple agent that counts down.")
        self._count = count

    @property
    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        return (TextMessage,)

    async def on_messages(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> Response:
        # Calls the on_messages_stream.
        response: Response | None = None
        async for message in self.on_messages_stream(messages, cancellation_token):
            if isinstance(message, Response):
                response = message
        assert response is not None
        return response

    async def on_messages_stream(
        self,
        messages: Sequence[BaseChatMessage],
        cancellation_token: CancellationToken,
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:
        inner_messages: List[BaseAgentEvent | BaseChatMessage] = []
        for i in range(self._count, 0, -1):
            msg = TextMessage(content=f"{i}...", source=self.name)
            inner_messages.append(msg)
            yield msg

        # The response is returned at the end of the stream.
        # It contains the final message and all the inner messages.
        yield Response(
            chat_message=TextMessage(content="Done!", source=self.name),
            inner_messages=inner_messages,
        )

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass

async def run_countdown_agent() -> None:
    # Create a countdown agent.
    countdown_agent = CountDownAgent("countdown")

    # Run the agent with a given task and stream the response.
    async for message in countdown_agent.on_messages_stream([], CancellationToken()):
        if isinstance(message, Response):
            print(message.chat_message)
        else:
            print(message)

# Use asyncio.run(run_countdown_agent()) when running in a script.
await run_countdown_agent()
```

Output:

```
3...
2...
1...
Done!
```


### ArithmeticAgent

To further illustrate custom agent capabilities, consider the `ArithmeticAgent`. This agent class is designed to perform basic arithmetic operations on a given integer. We can then instantiate multiple `ArithmeticAgent` instances, each configured for a different operation, and deploy them within a `SelectorGroupChat`. This setup allows for the transformation of an initial integer into a target integer through a sequence of chosen arithmetic operations.

The `ArithmeticAgent` class is initialized with an `operator_func`. This function is a callable that accepts an integer and, after applying a specific arithmetic operation, returns an integer. Within its `on_messages` method, the `ArithmeticAgent` applies this `operator_func` to the integer extracted from the input message, subsequently returning a `Response` object containing the computed result.

```python
from typing import Callable, Sequence
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.base import Response
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.messages import BaseChatMessage
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

class ArithmeticAgent(BaseChatAgent):
    def __init__(
        self, name: str, description: str, operator_func: Callable[[int], int]
    ) -> None:
        super().__init__(name, description=description)
        self._operator_func = operator_func
        self._message_history: List[BaseChatMessage] = []

    @property
    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        return (TextMessage,)

    async def on_messages(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> Response:
        # Update the message history.
        # NOTE: it is possible the messages is an empty list, which means the agent was selected previously.
        self._message_history.extend(messages)

        # Parse the number in the last message.
        assert isinstance(self._message_history[-1], TextMessage)
        number = int(self._message_history[-1].content)

        # Apply the operator function to the number.
        result = self._operator_func(number)

        # Create a new message with the result.
        response_message = TextMessage(content=str(result), source=self.name)

        # Update the message history.
        self._message_history.append(response_message)

        # Return the response.
        return Response(chat_message=response_message)

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass
```


It is crucial to note that the `on_messages` method might be invoked with an empty list of messages. This specific scenario indicates that the agent was previously engaged and is now being called upon again, but without any new messages originating from the caller. Consequently, maintaining a history of messages previously received by the agent becomes essential, as this history is leveraged to inform the generation of the subsequent response.

We can now proceed to instantiate a `SelectorGroupChat` populated with five distinct `ArithmeticAgent` instances, each configured for a unique operation:

*   One agent designed to increment the input integer by 1.
*   Another agent to decrement the input integer by 1.
*   An agent dedicated to multiplying the input integer by 2.
*   An agent that performs division of the input integer by 2, rounding down to the nearest integer.
*   And finally, an agent that simply returns the input integer without any modification.

These agents are then integrated into a `SelectorGroupChat`, configured with specific selector settings:

*   The `allow_repeated_speaker` setting is enabled (`True`), permitting the same agent to be chosen consecutively. This feature is particularly vital for tasks requiring repeated operations.
*   The `selector_prompt` is customized to precisely guide the model's selection process, tailoring its response to the requirements of the specific task.

```python
async def run_number_agents() -> None:
    # Create agents for number operations.
    add_agent = ArithmeticAgent("add_agent", "Adds 1 to the number.", lambda x: x + 1)
    multiply_agent = ArithmeticAgent(
        "multiply_agent", "Multiplies the number by 2.", lambda x: x * 2
    )
    subtract_agent = ArithmeticAgent(
        "subtract_agent", "Subtracts 1 from the number.", lambda x: x - 1
    )
    divide_agent = ArithmeticAgent(
        "divide_agent", "Divides the number by 2 and rounds down.", lambda x: x // 2
    )
    identity_agent = ArithmeticAgent(
        "identity_agent", "Returns the number as is.", lambda x: x
    )

    # The termination condition is to stop after 10 messages.
    termination_condition = MaxMessageTermination(10)

    # Create a selector group chat.
    selector_group_chat = SelectorGroupChat(
        [add_agent, multiply_agent, subtract_agent, divide_agent, identity_agent],
        model_client=OpenAIChatCompletionClient(model="gpt-4o"),
        termination_condition=termination_condition,
        allow_repeated_speaker=True,  # Allow the same agent to speak multiple times, necessary for this task.
        selector_prompt=(
            "Available roles: \n {roles} \n Their job descriptions: \n {participants} \n "
            "Current conversation history: \n {history} \n "
            "Please select the most appropriate role for the next message, and only return the role name."
        ),
    )

    # Run the selector group chat with a given task and stream the response.
    task: List[BaseChatMessage] = [
        TextMessage(content="Apply the operations to turn the given number into 25.", source="user"),
        TextMessage(content="10", source="user"),
    ]
    stream = selector_group_chat.run_stream(task=task)
    await Console(stream)

# Use asyncio.run(run_number_agents()) when running in a script.
await run_number_agents()
```

Output:

```
---------- user ----------
Apply the operations to turn the given number into 25.
---------- user ----------
10
---------- multiply_agent ----------
20
---------- add_agent ----------
21
---------- multiply_agent ----------
42
---------- divide_agent ----------
21
---------- add_agent ----------
22
---------- add_agent ----------
23
---------- add_agent ----------
24
---------- add_agent ----------
25
---------- Summary ----------
Number of messages: 10
Finish reason: Maximum number of messages 10 reached, current message count: 10
Total prompt tokens: 0
Total completion tokens: 0
Duration: 2.40 seconds
```


Observing the output, it is evident that the agents have successfully transformed the initial integer from 10 to 25. This was achieved by intelligently selecting and applying the appropriate arithmetic operations in sequence, demonstrating the dynamic capabilities of the `SelectorGroupChat`.

### Using Custom Model Clients in Custom Agents

A significant capability of the `AssistantAgent` preset within AgentChat is its direct acceptance of a `model_client` argument, which it then utilizes to formulate responses to messages. However, there are specific scenarios where an agent may require the use of a custom model client that is not currently supported by default, or necessitate custom behaviors from an existing model.

Such requirements can be effectively addressed by developing a custom agent that explicitly implements *your custom model client*.

Let's walk through an example of a custom agent that directly integrates and uses the Google Gemini SDK to respond to messages.

**Note:** To execute this example, you must install the Google Gemini SDK. This can be accomplished using the following command:

```bash
pip install google-genai
```

```python
import os
from typing import AsyncGenerator, Sequence
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.base import Response
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage
from autogen_core import CancellationToken
from autogen_core.model_context import UnboundedChatCompletionContext
from autogen_core.models import AssistantMessage, RequestUsage, UserMessage
from google import genai
from google.genai import types

class GeminiAssistantAgent(BaseChatAgent):
    def __init__(
        self,
        name: str,
        description: str = "An agent that provides assistance with ability to use tools.",
        model: str = "gemini-1.5-flash-002",
        api_key: str = os.environ["GEMINI_API_KEY"],
        system_message: str | None = "You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.",
    ):
        super().__init__(name=name, description=description)
        self._model_context = UnboundedChatCompletionContext()
        self._model_client = genai.Client(api_key=api_key)
        self._system_message = system_message
        self._model = model

    @property
    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        return (TextMessage,)

    async def on_messages(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> Response:
        final_response = None
        async for message in self.on_messages_stream(messages, cancellation_token):
            if isinstance(message, Response):
                final_response = message
        if final_response is None:
            raise AssertionError("The stream should have returned the final result.")
        return final_response

    async def on_messages_stream(
        self,
        messages: Sequence[BaseChatMessage],
        cancellation_token: CancellationToken,
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:
        # Add messages to the model context
        for msg in messages:
            await self._model_context.add_message(msg.to_model_message())

        # Get conversation history
        history = [
            (msg.source if hasattr(msg, "source") else "system")
            + ": "
            + (msg.content if isinstance(msg.content, str) else "")
            + " \n "
            for msg in await self._model_context.get_messages()
        ]

        # Generate response using Gemini
        response = self._model_client.models.generate_content(
            model=self._model,
            contents=f"History: {history} \n Given the history, please provide a response",
            config=types.GenerateContentConfig(
                system_instruction=self._system_message,
                temperature=0.3,
            ),
        )

        # Create usage metadata
        usage = RequestUsage(
            prompt_tokens=response.usage_metadata.prompt_token_count,
            completion_tokens=response.usage_metadata.candidates_token_count,
        )

        # Add response to model context
        await self._model_context.add_message(
            AssistantMessage(content=response.text, source=self.name)
        )

        # Yield the final response
        yield Response(
            chat_message=TextMessage(
                content=response.text, source=self.name, models_usage=usage
            ),
            inner_messages=[],
        )

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        """Reset the assistant by clearing the model context."""
        await self._model_context.clear()

gemini_assistant = GeminiAssistantAgent("gemini_assistant")
await Console(gemini_assistant.run_stream(task="What is the capital of New York?"))
```

Output:

```
---------- user ----------
What is the capital of New York?
---------- gemini_assistant ----------
Albany
TERMINATE
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='What is the capital of New York?', type='TextMessage'), TextMessage(source='gemini_assistant', models_usage=RequestUsage(prompt_tokens=46, completion_tokens=5), content='Albany\nTERMINATE\n', type='TextMessage')], stop_reason=None)
```


In the preceding example, we deliberately chose to expose `model`, `api_key`, and `system_message` as arguments within the `GeminiAssistantAgent`'s initialization. However, the design is flexible: you possess the autonomy to include any other arguments mandated by your specific model client or those that align seamlessly with your application's architectural design.

Now, let us explore how this custom agent can be seamlessly integrated as a contributing member within a team in AgentChat.

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")

# Create the primary agent.
primary_agent = AssistantAgent(
    "primary",
    model_client=model_client,
    system_message="You are a helpful AI assistant.",
)

# Create a critic agent based on our new GeminiAssistantAgent.
gemini_critic_agent = GeminiAssistantAgent(
    "gemini_critic",
    system_message="Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.",
)

# Define a termination condition that stops the task if the critic approves or after 10 messages.
termination = TextMentionTermination("APPROVE") | MaxMessageTermination(10)

# Create a team with the primary and critic agents.
team = RoundRobinGroupChat([primary_agent, gemini_critic_agent], termination_condition=termination)
await Console(team.run_stream(task="Write a Haiku poem with 4 lines about the fall season."))
await model_client.close()
```

Output:

```
---------- user ----------
Write a Haiku poem with 4 lines about the fall season.
---------- primary ----------
Crimson leaves cascade, Whispering winds sing of change, Chill wraps the fading, Nature's quilt, rich and warm.
---------- gemini_critic ----------
The poem is good, but it has four lines instead of three. A haiku must have three lines with a 5-7-5 syllable structure. The content is evocative of autumn, but the form is incorrect. Please revise to adhere to the haiku's syllable structure.
---------- primary ----------
Thank you for your feedback! Here’s a revised haiku that follows the 5-7-5 syllable structure: Crimson leaves drift down, Chill winds whisper through the gold, Autumn’s breath is near.
---------- gemini_critic ----------
The revised haiku is much improved. It correctly follows the 5-7-5 syllable structure and maintains the evocative imagery of autumn. APPROVE
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a Haiku poem with 4 lines about the fall season.', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=33, completion_tokens=31), content="Crimson leaves cascade, \nWhispering winds sing of change, \nChill wraps the fading, \nNature's quilt, rich and warm.", type='TextMessage'), TextMessage(source='gemini_critic', models_usage=RequestUsage(prompt_tokens=86, completion_tokens=60), content="The poem is good, but it has four lines instead of three. A haiku must have three lines with a 5-7-5 syllable structure. The content is evocative of autumn, but the form is incorrect. Please revise to adhere to the haiku's syllable structure.\n", type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=141, completion_tokens=49), content='Thank you for your feedback! Here’s a revised haiku that follows the 5-7-5 syllable structure:\n\nCrimson leaves drift down, \nChill winds whisper through the gold, \nAutumn’s breath is near.', type='TextMessage'), TextMessage(source='gemini_critic', models_usage=RequestUsage(prompt_tokens=211, completion_tokens=32), content='The revised haiku is much improved. It correctly follows the 5-7-5 syllable structure and maintains the evocative imagery of autumn. APPROVE\n', type='TextMessage')], stop_reason="Text 'APPROVE' mentioned")
```


The preceding section effectively showcases several critical concepts:

*   We successfully developed a custom agent that harnesses the Google Gemini SDK for message responses.
*   It was demonstrated that this custom agent can be seamlessly integrated into the broader AgentChat ecosystem. Specifically, it functioned as a participant in a `RoundRobinGroupChat`, a capability ensured by its inheritance from `BaseChatAgent`.

### Making the Custom Agent Declarative

AutoGen provides a `Component` interface, a powerful mechanism for making the configuration of components serializable into a declarative format. This functionality is immensely valuable for a variety of use cases, including saving and loading configurations, as well as facilitating the sharing of configurations among different users or systems.

This is accomplished by inheriting from the `Component` class and implementing two specific methods: `_from_config` and `_to_config`. A class designed with this declarative capability can be serialized into a JSON format using the `dump_component` method, and subsequently deserialized back from a JSON format using the `load_component` method.

```python
import os
from typing import AsyncGenerator, Sequence
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.base import Response
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage
from autogen_core import CancellationToken, Component
from pydantic import BaseModel
from typing_extensions import Self

class GeminiAssistantAgentConfig(BaseModel):
    name: str
    description: str = "An agent that provides assistance with ability to use tools."
    model: str = "gemini-1.5-flash-002"
    system_message: str | None = None

class GeminiAssistantAgent(BaseChatAgent, Component[GeminiAssistantAgentConfig]):
    # type: ignore[no-redef]
    component_config_schema = GeminiAssistantAgentConfig
    # component_provider_override = "mypackage.agents.GeminiAssistantAgent"

    def __init__(
        self,
        name: str,
        description: str = "An agent that provides assistance with ability to use tools.",
        model: str = "gemini-1.5-flash-002",
        api_key: str = os.environ["GEMINI_API_KEY"],
        system_message: str | None = "You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.",
    ):
        super().__init__(name=name, description=description)
        self._model_context = UnboundedChatCompletionContext()
        self._model_client = genai.Client(api_key=api_key)
        self._system_message = system_message
        self._model = model

    @property
    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        return (TextMessage,)

    async def on_messages(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> Response:
        final_response = None
        async for message in self.on_messages_stream(messages, cancellation_token):
            if isinstance(message, Response):
                final_response = message
        if final_response is None:
            raise AssertionError("The stream should have returned the final result.")
        return final_response

    async def on_messages_stream(
        self,
        messages: Sequence[BaseChatMessage],
        cancellation_token: CancellationToken,
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:
        # Add messages to the model context
        for msg in messages:
            await self._model_context.add_message(msg.to_model_message())

        # Get conversation history
        history = [
            (msg.source if hasattr(msg, "source") else "system")
            + ": "
            + (msg.content if isinstance(msg.content, str) else "")
            + " \n "
            for msg in await self._model_context.get_messages()
        ]

        # Generate response using Gemini
        response = self._model_client.models.generate_content(
            model=self._model,
            contents=f"History: {history} \n Given the history, please provide a response",
            config=types.GenerateContentConfig(
                system_instruction=self._system_message,
                temperature=0.3,
            ),
        )

        # Create usage metadata
        usage = RequestUsage(
            prompt_tokens=response.usage_metadata.prompt_token_count,
            completion_tokens=response.usage_metadata.candidates_token_count,
        )

        # Add response to model context
        await self._model_context.add_message(
            AssistantMessage(content=response.text, source=self.name)
        )

        # Yield the final response
        yield Response(
            chat_message=TextMessage(
                content=response.text, source=self.name, models_usage=usage
            ),
            inner_messages=[],
        )

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        """Reset the assistant by clearing the model context."""
        await self._model_context.clear()

    @classmethod
    def _from_config(cls, config: GeminiAssistantAgentConfig) -> Self:
        return cls(
            name=config.name,
            description=config.description,
            model=config.model,
            system_message=config.system_message
        )

    def _to_config(self) -> GeminiAssistantAgentConfig:
        return GeminiAssistantAgentConfig(
            name=self.name,
            description=self.description,
            model=self._model,
            system_message=self._system_message,
        )
```


With the essential methods now implemented, we can seamlessly serialize and deserialize the custom agent to and from a JSON format.

**Note:** It is imperative to set the `component_provider_override` class variable to the full path of the module containing your custom agent class (e.g., `mypackage.agents.GeminiAssistantAgent`). This variable is crucial for the `load_component` method, as it guides the framework on how to correctly instantiate the class during deserialization.

```python
gemini_assistant = GeminiAssistantAgent("gemini_assistant")
config = gemini_assistant.dump_component()
print(config.model_dump_json(indent=2))
loaded_agent = GeminiAssistantAgent.load_component(config)
print(loaded_agent)
```

Output:

```json
{
  "provider": "__main__.GeminiAssistantAgent",
  "component_type": "agent",
  "version": 1,
  "component_version": 1,
  "description": null,
  "label": "GeminiAssistantAgent",
  "config": {
    "name": "gemini_assistant",
    "description": "An agent that provides assistance with ability to use tools.",
    "model": "gemini-1.5-flash-002",
    "system_message": "You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed."
  }
}
<__main__.GeminiAssistantAgent object at 0x11a5c5a90>
```

### Next Steps

To recap, we've explored the fundamental processes of creating custom agents, integrating custom model clients within these agents, and making them declarative for easier configuration management. This foundational example can be expanded upon in several beneficial ways:

*   **Enhance the Gemini model client:** Consider extending its capabilities to support function calling, mirroring the functionality found in the `AssistantAgent` class. Comprehensive documentation on Gemini API function calling is available at [https://ai.google.dev/gemini-api/docs/function-calling](https://ai.google.dev/gemini-api/docs/function-calling).
*   **Develop a package with a custom agent:** Package your custom agent and then experiment with its declarative format within tools like AutoGen Studio. This allows for more structured development and sharing of agents.